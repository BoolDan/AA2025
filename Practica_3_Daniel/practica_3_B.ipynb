{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Parte B***: Redes Neuronales Convolucionales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch torchvision matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Preguntas***\n",
    "1. ¿Que significa que el batch_size sea 4? Explica cómo afectaría al cálculo de los pesos en cada epoch.\n",
    "La variable batch_size define cuantos datos se procesan en cada lote, al tener un valor pequeño el calculo de los pesos se realiza más veces en cada epoch.\n",
    "\n",
    "2. ¿Por qué al dividir el conjunto de entrenamiento el parámetro shuffle está a True, mientras que el conjunto de test está a False?\n",
    "El parametro shuffle reorganiza de manera aleatoria los datos que se cargan en cada epoch, esto puede ser beneficioso para evitar patrones que puedan afectar al desempeño del modelo, en el test no es necesario ya que son los datos de validación y por lo tanto no es necesario hacer el shuffle.\n",
    "\n",
    "3. Ver en la celda de código adyacente.\n",
    "\n",
    "4. Indica la línea de código donde se realiza el cálculo de los gradientes para determinar cómo\n",
    "modificar los pesos (los filtros kernel), e indica la línea de código donde se recalculan los pesos.\n",
    "¿Qué se ha tenido que realizar antes para poder hacer el cálculo de gradientes y la\n",
    "actualización de pesos?\n",
    "\n",
    "5. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Pregunta 3\n",
    "'''\n",
    "class Net(nn.Module):  # Define una clase para la red neuronal que hereda de nn.Module\n",
    "    def __init__(self):  # Constructor de la clase\n",
    "        super().__init__()  # Llama al constructor de la clase base nn.Module\n",
    "        # Primera capa convolucional: toma imágenes con 3 canales (RGB), aplica 6 filtros de tamaño 5x5\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)  \n",
    "        # Capa de pooling: aplica max pooling con un tamaño de ventana 2x2 y un paso (stride) de 2\n",
    "        self.pool = nn.MaxPool2d(2, 2)  \n",
    "        # Segunda capa convolucional: toma 6 canales de entrada y aplica 16 filtros de tamaño 5x5\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)  \n",
    "        # Primera capa completamente conectada: conecta 16 * 5 * 5 entradas (salida de la convolución) a 120 neuronas\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  \n",
    "        # Segunda capa completamente conectada: conecta 120 neuronas a 84 neuronas\n",
    "        self.fc2 = nn.Linear(120, 84)  \n",
    "        # Tercera capa completamente conectada: conecta 84 neuronas a 10 neuronas (una por cada clase)\n",
    "        self.fc3 = nn.Linear(84, 10)  \n",
    "\n",
    "    def forward(self, x):  # Define el flujo de datos a través de la red (forward propagation)\n",
    "        # Aplica la primera capa convolucional, seguida de ReLU y max pooling\n",
    "        x = self.pool(F.relu(self.conv1(x)))  \n",
    "        # Aplica la segunda capa convolucional, seguida de ReLU y max pooling\n",
    "        x = self.pool(F.relu(self.conv2(x)))  \n",
    "        # Aplana el tensor (convierte las dimensiones de características en una sola dimensión, excepto el batch)\n",
    "        x = torch.flatten(x, 1)  # Mantiene la dimensión del batch (dimensión 0)\n",
    "        # Aplica la primera capa completamente conectada con activación ReLU\n",
    "        x = F.relu(self.fc1(x))  \n",
    "        # Aplica la segunda capa completamente conectada con activación ReLU\n",
    "        x = F.relu(self.fc2(x))  \n",
    "        # Aplica la última capa completamente conectada (sin activación, ya que es la salida final)\n",
    "        x = self.fc3(x)  \n",
    "        return x  # Devuelve la salida final (logits para las 10 clases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#Declaramos las dos redes neuronales que vamos a utilizar\n",
    "class Net1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "class Net2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Primera capa convolucional: 3 canales de entrada (RGB), 8 filtros, tamaño de kernel 3x3\n",
    "        self.conv1 = nn.Conv2d(1, 20, kernel_size=5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)  # Capa de pooling: tamaño 2x2, stride 2\n",
    "        # Segunda capa convolucional: 8 canales de entrada, 20 filtros, tamaño de kernel 3x3\n",
    "        self.conv2 = nn.Conv2d(20, 20, kernel_size=5)\n",
    "        # Ajuste de la capa completamente conectada para las nuevas dimensiones\n",
    "        self.fc1 = nn.Linear(20 * 5 * 5, 120)  # 20 filtros, tamaño 5x5 después de convoluciones y pooling\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # Primera capa convolucional + ReLU + pooling\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # Segunda capa convolucional + ReLU + pooling\n",
    "        x = torch.flatten(x, 1)  # Aplanar todas las dimensiones excepto el batch\n",
    "        x = F.relu(self.fc1(x))  # Primera capa completamente conectada + ReLU\n",
    "        x = F.relu(self.fc2(x))  # Segunda capa completamente conectada + ReLU\n",
    "        x = self.fc3(x)  # Capa de salida\n",
    "        return x\n",
    "    \n",
    "  \n",
    "net1 = Net1()\n",
    "net2 = Net2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer1 = optim.SGD(net1.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer2 = optim.SGD(net2.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer1.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net1(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer1.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer2.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net2(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer2.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
